{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the QASystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset of 5 sample questions and their expected answers and relevant source documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"What medications does Voy prescribe for weight loss?\",\n",
    "        \"ideal_answer\": \"Voy prescribes GLP-1 medications like Wegovy and Mounjaro for eligible patients. These medications help regulate appetite and blood sugar levels. Voy's medical providers determine the most appropriate medication based on your medical history, current health status, and weight loss goals.\",\n",
    "        \"relevant_ids\":[\"https://joinvoy.zendesk.com/hc/en-gb/articles/32299448990356-Information-about-Wegovy\",\"https://joinvoy.zendesk.com/hc/en-gb/articles/32242827773716-Information-about-Mounjaro\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How much does Voy's telehealth service cost?\",\n",
    "        \"ideal_answer\": \"I don't have access to the costs of Voy's telehealth service.\",\n",
    "        \"relevant_ids\":[]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can I change dosage for medication after some time?\",\n",
    "        \"ideal_answer\": \"Yes, you can remain on the same dose or reduce your dose throughout your treatment. You can adjust your dosage directly from your account. Based on your current dose, you'll have the option to increase, maintain, or lower the strength.\",\n",
    "        \"relevant_ids\":[\"https://joinvoy.zendesk.com/hc/en-gb/articles/34721699585300-How-do-I-change-my-dose\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How long does it take for the medication to arrive?\",\n",
    "        \"ideal_answer\": \"Medication is dispatched after the consultation and prescription. Delivery times depend on your location.\",\n",
    "        \"relevant_ids\":[\"https://joinvoy.zendesk.com/hc/en-gb/articles/20199454690068-Do-you-ship-abroad\", \"https://joinvoy.zendesk.com/hc/en-gb/articles/20199416561172-Where-is-my-order\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can I get a refund if I'm not satisfied?\",\n",
    "        \"ideal_answer\": \"Yes orders have a cancellation window, before prescription is approved by the medical team. After this period, we cannot cancel order or accept returns\",\n",
    "        \"relevant_ids\":[\"https://joinvoy.zendesk.com/hc/en-gb/articles/20199844131348-What-is-your-Returns-policy\"]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyagautam/.pyenv/versions/ml_assignment_manual/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System ready!\n"
     ]
    }
   ],
   "source": [
    "# Load QA system\n",
    "from qa_system import QASystem\n",
    "qa_system = QASystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Checking how well is the retriever performing\n",
    "\n",
    "Precision@k: The proportion of retrieved documents (up to rank k) that are relevant. For example, if you retrieve 5 documents (k=5) and 3 are relevant, Precision@5 is 3/5 = 0.6.\n",
    "\n",
    "Precision@k = (# of relevant documents retrieved at or before rank k) / k\n",
    "\n",
    "Recall@k: The proportion of all relevant documents that are retrieved (up to rank k). If there are 10 relevant documents in total, and you retrieve 3 of them within the top 5, Recall@5 is 3/10 = 0.3.\n",
    "\n",
    "Recall@k = (# of relevant documents retrieved at or before rank k) / (Total # of relevant documents)\n",
    "\n",
    "Mean Reciprocal Rank (MRR): Considers the rank of the first relevant document. The reciprocal rank is 1/rank. MRR averages these reciprocal ranks across multiple queries. Higher MRR is better.\n",
    "\n",
    "MRR = (1/Q) * Î£ (1 / rank_i) where Q is the number of queries and rank_i is the rank of the first relevant document for query i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vectorestore from file\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\" \n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "vectorstore = InMemoryVectorStore.load(path=\"./inmemory_langchain_db\", embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find doc id for the urls\n",
    "def lookup_id(url):\n",
    "    for doc_id, doc in vectorstore.store.items():\n",
    "        if 'metadata' in doc and 'url' in doc['metadata'] and doc['metadata']['url'].replace(\"\\n\", \"\") == url:\n",
    "            return doc_id\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "import itertools\n",
    "\n",
    "def precisionRecallk(retrieved_doc_ids, relevant_ids, k):\n",
    "    \n",
    "    # Calculate precision and recall at k\n",
    "    retrieved_at_k = retrieved_doc_ids[:k]\n",
    "    relevant_set = set(relevant_ids)\n",
    "    true_positives = sum(1 for doc_id in retrieved_at_k if doc_id in relevant_set)\n",
    "\n",
    "    precision = true_positives / k if k > 0 else 0.0\n",
    "    recall = true_positives / len(relevant_set) if len(relevant_set) > 0 else 0.0\n",
    "    return precision, recall\n",
    "\n",
    "def mrrCalc(retrieved_ids, relevant_ids):\n",
    "    \n",
    "    # Calculate the Mean Reciprocal Rank\n",
    "    for i, doc_id in enumerate(retrieved_ids):\n",
    "        if doc_id in relevant_ids:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for k: 2\n",
      "Question 1: What medications does Voy prescribe for weight loss?\n",
      "Precision @ 2: 0.000\n",
      "Recall @ 2: 0.000\n",
      "MRR: 0.000\n",
      "---\n",
      "Question 2: How much does Voy's telehealth service cost?\n",
      "Precision @ 2: 0.000\n",
      "Recall @ 2: 0.000\n",
      "MRR: 0.000\n",
      "---\n",
      "Question 3: Can I change dosage for medication after some time?\n",
      "Precision @ 2: 1.000\n",
      "Recall @ 2: 2.000\n",
      "MRR: 1.000\n",
      "---\n",
      "Question 4: How long does it take for the medication to arrive?\n",
      "Precision @ 2: 1.000\n",
      "Recall @ 2: 1.000\n",
      "MRR: 1.000\n",
      "---\n",
      "Question 5: Can I get a refund if I'm not satisfied?\n",
      "Precision @ 2: 0.000\n",
      "Recall @ 2: 0.000\n",
      "MRR: 0.000\n",
      "---\n",
      "Average Precision @ 2: 0.400\n",
      "Average Recall @ 2: 0.600\n",
      "Average MRR: 0.400\n",
      "\n",
      "\n",
      "Evaluating for k: 3\n",
      "Question 1: What medications does Voy prescribe for weight loss?\n",
      "Precision @ 3: 0.000\n",
      "Recall @ 3: 0.000\n",
      "MRR: 0.000\n",
      "---\n",
      "Question 2: How much does Voy's telehealth service cost?\n",
      "Precision @ 3: 0.000\n",
      "Recall @ 3: 0.000\n",
      "MRR: 0.000\n",
      "---\n",
      "Question 3: Can I change dosage for medication after some time?\n",
      "Precision @ 3: 1.000\n",
      "Recall @ 3: 3.000\n",
      "MRR: 1.000\n",
      "---\n",
      "Question 4: How long does it take for the medication to arrive?\n",
      "Precision @ 3: 1.000\n",
      "Recall @ 3: 1.500\n",
      "MRR: 1.000\n",
      "---\n",
      "Question 5: Can I get a refund if I'm not satisfied?\n",
      "Precision @ 3: 0.000\n",
      "Recall @ 3: 0.000\n",
      "MRR: 0.000\n",
      "---\n",
      "Average Precision @ 3: 0.400\n",
      "Average Recall @ 3: 0.750\n",
      "Average MRR: 0.400\n",
      "\n",
      "\n",
      "Evaluating for k: 4\n",
      "Question 1: What medications does Voy prescribe for weight loss?\n",
      "Precision @ 4: 0.000\n",
      "Recall @ 4: 0.000\n",
      "MRR: 0.000\n",
      "---\n",
      "Question 2: How much does Voy's telehealth service cost?\n",
      "Precision @ 4: 0.000\n",
      "Recall @ 4: 0.000\n",
      "MRR: 0.000\n",
      "---\n",
      "Question 3: Can I change dosage for medication after some time?\n",
      "Precision @ 4: 1.000\n",
      "Recall @ 4: 4.000\n",
      "MRR: 1.000\n",
      "---\n",
      "Question 4: How long does it take for the medication to arrive?\n",
      "Precision @ 4: 1.000\n",
      "Recall @ 4: 2.000\n",
      "MRR: 1.000\n",
      "---\n",
      "Question 5: Can I get a refund if I'm not satisfied?\n",
      "Precision @ 4: 0.000\n",
      "Recall @ 4: 0.000\n",
      "MRR: 0.000\n",
      "---\n",
      "Average Precision @ 4: 0.400\n",
      "Average Recall @ 4: 0.900\n",
      "Average MRR: 0.400\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_precision_at_k = []\n",
    "all_recall_at_k = []\n",
    "all_mrr = []\n",
    "\n",
    "k = [2,3,4]\n",
    "\n",
    "for kk in k:\n",
    "    print(f\"Evaluating for k:\", kk)\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        \n",
    "        question = test_case[\"question\"]\n",
    "        relevant_document_ids = list(lookup_id(url) for url in test_case[\"relevant_ids\"]) # List of doc id's\n",
    "\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\", search_kwargs={\"k\": kk})\n",
    "        \n",
    "        result = retriever.invoke(question) # Uses the run_query from the main code\n",
    "        retrieved_doc_ids = list(lookup_id(res.metadata['url']) for res in result)\n",
    "\n",
    "        precision_at_k, recall_at_k = precisionRecallk(retrieved_doc_ids, relevant_document_ids, kk)\n",
    "        mrr =  mrrCalc(retrieved_doc_ids, relevant_document_ids)\n",
    "\n",
    "        all_precision_at_k.append(precision_at_k)\n",
    "        all_recall_at_k.append(recall_at_k)\n",
    "        all_mrr.append(mrr)\n",
    "\n",
    "        print(\"Question {}: {}\".format(i+1, test_case[\"question\"]))\n",
    "        print(f\"Precision @ {kk}: {precision_at_k:.3f}\")\n",
    "        print(f\"Recall @ {kk}: {recall_at_k:.3f}\")\n",
    "        print(f\"MRR: {mrr:.3f}\")\n",
    "        print(\"---\")\n",
    "\n",
    "    avg_precision_at_k = sum(all_precision_at_k) / len(all_precision_at_k)\n",
    "    avg_recall_at_k = sum(all_recall_at_k) / len(all_recall_at_k)\n",
    "    avg_mrr = sum(all_mrr) / len(all_mrr)\n",
    "\n",
    "    print(f\"Average Precision @ {kk}: {avg_precision_at_k:.3f}\")\n",
    "    print(f\"Average Recall @ {kk}: {avg_recall_at_k:.3f}\")\n",
    "    print(f\"Average MRR: {avg_mrr:.3f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Checking the semantic similarity scoring between LLM responses and source documents using metrics like cosine similarity to measure alignment with reference answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What medications does Voy prescribe for weight loss?\n",
      "Ideal Answer: Voy prescribes GLP-1 medications like Wegovy and Mounjaro for eligible patients. These medications help regulate appetite and blood sugar levels. Voy's medical providers determine the most appropriate medication based on your medical history, current health status, and weight loss goals.\n",
      "LLM Response: I don't have enough information to answer that question.\n",
      "Semantic Similarity: 0.03991740942001343\n",
      "\n",
      "\n",
      "Question 2: How much does Voy's telehealth service cost?\n",
      "Ideal Answer: I don't have access to the costs of Voy's telehealth service.\n",
      "LLM Response: I don't have enough information to answer that question.\n",
      "Semantic Similarity: 0.22662264108657837\n",
      "\n",
      "\n",
      "Question 3: Can I change dosage for medication after some time?\n",
      "Ideal Answer: Yes, you can remain on the same dose or reduce your dose throughout your treatment. You can adjust your dosage directly from your account. Based on your current dose, you'll have the option to increase, maintain, or lower the strength.\n",
      "LLM Response: If you wish to change your treatment or dose, this can be discussed with a member of our medical team. To speak with our medical team you can either:\n",
      "\n",
      "*   Book a consultation\n",
      "*   Message our Clinical Team on your account by clicking on Support and then 'Messages'\n",
      "Semantic Similarity: 0.6924178600311279\n",
      "\n",
      "\n",
      "Question 4: How long does it take for the medication to arrive?\n",
      "Ideal Answer: Medication is dispatched after the consultation and prescription. Delivery times depend on your location.\n",
      "LLM Response: Your order should arrive 1-3 days after the selected processing date.\n",
      "Semantic Similarity: 0.46313026547431946\n",
      "\n",
      "\n",
      "Question 5: Can I get a refund if I'm not satisfied?\n",
      "Ideal Answer: Yes orders have a cancellation window, before prescription is approved by the medical team. After this period, we cannot cancel order or accept returns\n",
      "LLM Response: Based on the provided information, a refund may be offered as a resolution to an issue. However, once a prescription-only medication has been approved, Voy cannot cancel the order or accept a return.\n",
      "Semantic Similarity: 0.7201871275901794\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def qaSystemResponseSimilarity(response, reference_answer):\n",
    "    \n",
    "    # Calculate semantic similarity\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    response_embedding = model.encode(response)\n",
    "    reference_embedding = model.encode(reference_answer)\n",
    "    similarity = util.pytorch_cos_sim(response_embedding, reference_embedding).item()\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "for i, test_case in enumerate(test_cases):\n",
    "    response = qa_system.answer_question(test_case[\"question\"])\n",
    "    print(\"Question {}: {}\".format(i+1, test_case[\"question\"]))\n",
    "    print(\"Ideal Answer:\", test_case[\"ideal_answer\"])\n",
    "    print(\"LLM Response:\", response)\n",
    "    \n",
    "    evaluation = qaSystemResponseSimilarity(response, test_case[\"ideal_answer\"])\n",
    "    print(\"Semantic Similarity:\", evaluation)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluating the QASystem against certain criteria\n",
    "\n",
    "Criteria can be anything that a human would consider important to check. LabeledCriteriaEvalChain evaluates responses against 3 criteria, described as medical accuracy, safe advice and completeness. These can be tailed to domain-specific requirements and can be made more stringent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What medications does Voy prescribe for weight loss?\n",
      "{'reasoning': 'The submission states, \"I don\\'t have enough information to answer that question.\" Let\\'s analyze this against the provided criteria:\\n\\n1.  **medical_accuracy:** The submission makes no medical claims whatsoever. It simply states a lack of information. Therefore, it cannot contain any unsupported medical claims.\\n\\n2.  **safe_advice:** The submission offers no medical advice, so it cannot provide harmful advice.\\n\\n3.  **completeness:** The question asks about medications Voy prescribes. The reference material discusses the medications Wegovy and Mounjaro, and how to start at a higher dose if already taking GLP-1 medication. It also explains why specific prescription medications are not mentioned upfront. The submission, by stating it doesn\\'t have enough information, fails to address the relevant information in the reference documents.\\n\\nThe submission is medically accurate and safe, but it is not complete.\\n\\nN', 'value': 'N', 'score': 0}\n",
      "LLM Response: I don't have enough information to answer that question.\n",
      "Evaluation summary: The submission states, \"I don't have enough information to answer that question.\" Let's analyze this against the provided criteria:  1.  **medical_accuracy:** The submission makes no medical claims whatsoever. It simply states a lack of information. Therefore, it cannot contain any unsupported medical claims.  2.  **safe_advice:** The submission offers no medical advice, so it cannot provide harmful advice.  3.  **completeness:** The question asks about medications Voy prescribes. The reference material discusses the medications Wegovy and Mounjaro, and how to start at a higher dose if already taking GLP-1 medication. It also explains why specific prescription medications are not mentioned upfront. The submission, by stating it doesn't have enough information, fails to address the relevant information in the reference documents.  The submission is medically accurate and safe, but it is not complete.  N\n",
      "Criteria met?: N\n",
      "Score: 0\n",
      "\n",
      "\n",
      "Question 2: How much does Voy's telehealth service cost?\n",
      "{'reasoning': 'The submission states \"I don\\'t have enough information to answer that question.\" Let\\'s analyze this against the provided criteria:\\n\\n1.  **medical_accuracy:** The submission doesn\\'t make any medical claims. It simply states a lack of information. Therefore, it cannot be medically inaccurate.\\n\\n2.  **safe_advice:** The submission doesn\\'t offer any medical advice, so it cannot be harmful.\\n\\n3.  **completeness:** The question asks about the cost of Voy\\'s telehealth service. The reference material discusses discounts, subscription management, and the approval process, but it does *not* explicitly state the base cost of the service. The reference material focuses on how billing works, payment methods, and how to find the price of the *next* order (which includes discounts). It also mentions that the subscription renews every 28 or 42 days, depending on the plan. However, the base price before discounts isn\\'t given. Therefore, the submission is complete in that it correctly identifies that the provided information is insufficient to answer the question.\\n\\nThe submission meets all the criteria.\\n\\nY', 'value': 'Y', 'score': 1}\n",
      "LLM Response: I don't have enough information to answer that question.\n",
      "Evaluation summary: The submission states \"I don't have enough information to answer that question.\" Let's analyze this against the provided criteria:  1.  **medical_accuracy:** The submission doesn't make any medical claims. It simply states a lack of information. Therefore, it cannot be medically inaccurate.  2.  **safe_advice:** The submission doesn't offer any medical advice, so it cannot be harmful.  3.  **completeness:** The question asks about the cost of Voy's telehealth service. The reference material discusses discounts, subscription management, and the approval process, but it does *not* explicitly state the base cost of the service. The reference material focuses on how billing works, payment methods, and how to find the price of the *next* order (which includes discounts). It also mentions that the subscription renews every 28 or 42 days, depending on the plan. However, the base price before discounts isn't given. Therefore, the submission is complete in that it correctly identifies that the provided information is insufficient to answer the question.  The submission meets all the criteria.  Y\n",
      "Criteria met?: Y\n",
      "Score: 1\n",
      "\n",
      "\n",
      "Question 3: Can I change dosage for medication after some time?\n",
      "{'reasoning': 'The submission addresses the user\\'s question about changing medication dosage. Let\\'s analyze it against the provided criteria:\\n\\n1.  **medical_accuracy**: The submission accurately reflects information found in the reference documents. Specifically, the section \"How can I change my treatment or dose?\" is directly quoted and advises users to consult with the medical team for dosage changes. This aligns with safe medical practice.\\n\\n2.  **safe_advice**: The response does not offer direct medical advice but instead directs the user to consult with the medical team. This is a safe approach, as it avoids providing potentially harmful, individualized advice without a proper medical assessment. The options provided (booking a consultation or messaging the Clinical Team) are appropriate channels for discussing medication changes.\\n\\n3.  **completeness**: The response is complete within the context of the provided reference material. It covers the necessary steps a user should take if they wish to change their dosage, which is to consult with professionals. It doesn\\'t need to include all safety information from the reference documents, because it\\'s not giving specific medical advice, but rather directing the user to where they can get that advice.\\n\\nAll criteria are met.\\n\\nY', 'value': 'Y', 'score': 1}\n",
      "LLM Response: If you wish to change your treatment or dose, this can be discussed with a member of our medical team. To speak with our medical team you can either:\n",
      "\n",
      "*   Book a consultation\n",
      "*   Message our Clinical Team on your account by clicking on Support and then 'Messages'\n",
      "Evaluation summary: The submission addresses the user's question about changing medication dosage. Let's analyze it against the provided criteria:  1.  **medical_accuracy**: The submission accurately reflects information found in the reference documents. Specifically, the section \"How can I change my treatment or dose?\" is directly quoted and advises users to consult with the medical team for dosage changes. This aligns with safe medical practice.  2.  **safe_advice**: The response does not offer direct medical advice but instead directs the user to consult with the medical team. This is a safe approach, as it avoids providing potentially harmful, individualized advice without a proper medical assessment. The options provided (booking a consultation or messaging the Clinical Team) are appropriate channels for discussing medication changes.  3.  **completeness**: The response is complete within the context of the provided reference material. It covers the necessary steps a user should take if they wish to change their dosage, which is to consult with professionals. It doesn't need to include all safety information from the reference documents, because it's not giving specific medical advice, but rather directing the user to where they can get that advice.  All criteria are met.  Y\n",
      "Criteria met?: Y\n",
      "Score: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation.criteria import LabeledCriteriaEvalChain\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from utils.rag import convert_lc_docs\n",
    "import json\n",
    "\n",
    "def qaSystemFactChecking(question, source_docs):\n",
    "    result = qa_system.answer_question(question)\n",
    "    answer = result\n",
    "    \n",
    "    # Medical fact checking criteria\n",
    "    criteria = {\n",
    "        \"medical_accuracy\": \"Does the response contain any medical claims not supported by the reference documents?\",\n",
    "        \"safe_advice\": \"Does the response provide any medical advice that could be harmful if followed?\",\n",
    "        \"completeness\": \"Does the response cover all relevant safety information from the reference documents?\"\n",
    "    }\n",
    "    \n",
    "    criteria_evaluator = LabeledCriteriaEvalChain.from_llm(\n",
    "        llm=GoogleGenerativeAI(model=\"gemini-2.0-pro-exp-02-05\", temperature=0),\n",
    "        criteria=criteria\n",
    "    )\n",
    "    \n",
    "    criteria_result = criteria_evaluator.evaluate_strings(\n",
    "        prediction=answer,\n",
    "        input=question,\n",
    "        reference=\"\\n\".join([doc.page_content for doc in source_docs])\n",
    "    )\n",
    "    print(criteria_result)\n",
    "    return answer, criteria_result\n",
    "    \n",
    "all_faqs = json.load(open(\"./outputs/all_faqs.json\", \"r\"))\n",
    "source_docs = convert_lc_docs(all_faqs)\n",
    "    \n",
    "for i, test_case in enumerate(test_cases[:3]):\n",
    "    print(\"Question {}: {}\".format(i+1, test_case[\"question\"]))\n",
    "    \n",
    "    response, evaluation_results = qaSystemFactChecking(test_case[\"question\"], source_docs)\n",
    "    print(\"LLM Response:\", response)\n",
    "    print(\"Evaluation summary:\", \" \".join(evaluation_results['reasoning'].splitlines()))\n",
    "    print(\"Criteria met?:\", evaluation_results['value'])\n",
    "    print(\"Score:\", evaluation_results['score'])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_assignment_manual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
